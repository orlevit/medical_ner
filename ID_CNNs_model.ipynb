{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6fce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.util import minibatch,compounding\n",
    "from spacy.training import Example\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from config import STRUBELL_MODEL_OUTPUT_FILE\n",
    "from helper import read_train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def convert_to_spacy_format(texts, entities_lists):\n",
    "    \"\"\"\n",
    "    Convert text and entity annotations to spaCy training format.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings\n",
    "        entities_lists: List of dictionaries containing entity annotations\n",
    "        \n",
    "    Returns:\n",
    "        List of (text, annotations) tuples in spaCy format\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    valid_entity_types = {\"Condition\", \"Procedure\", \"Medication\"}\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        candidate_spans = []\n",
    "        for entity_type, entity_list in entities_lists[i].items():\n",
    "            if entity_type not in valid_entity_types:\n",
    "                continue\n",
    "                \n",
    "            for entity in entity_list:\n",
    "                if not entity:\n",
    "                    continue\n",
    "                    \n",
    "                entity_lower = entity.lower()\n",
    "                text_lower = text.lower()\n",
    "                \n",
    "                for match in re.finditer(re.escape(entity_lower), text_lower):\n",
    "                    start, end = match.span()\n",
    "                    candidate_spans.append((start, end, entity_type, end - start))\n",
    "        \n",
    "        candidate_spans.sort(key=lambda x: x[3], reverse=True)\n",
    "        \n",
    "        final_spans = []\n",
    "        token_occupancy = set()\n",
    "        \n",
    "        for start, end, entity_type, _ in candidate_spans:\n",
    "            span_positions = set(range(start, end))\n",
    "            if span_positions.intersection(token_occupancy):\n",
    "                continue\n",
    "            \n",
    "            final_spans.append((start, end, entity_type))\n",
    "            token_occupancy.update(span_positions)\n",
    "        \n",
    "        final_spans.sort(key=lambda x: x[0])\n",
    "        \n",
    "        training_data.append((text, {\"entities\": final_spans}))\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "def train_spacy_model(model, train_data, iterations=30):\n",
    "    \"\"\"\n",
    "    Train a spaCy NER model with the provided training data.\n",
    "    \n",
    "    Args:\n",
    "        model: spaCy model to train\n",
    "        train_data: Training data in spaCy format\n",
    "        iterations: Number of training iterations\n",
    "        \n",
    "    Returns:\n",
    "        Trained spaCy model\n",
    "    \"\"\"\n",
    "    losses = {}\n",
    "    \n",
    "    if \"ner\" not in model.pipe_names:\n",
    "        ner = model.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = model.get_pipe(\"ner\")\n",
    "        \n",
    "    added_labels = set()\n",
    "    for text, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\", []):\n",
    "            if len(ent) == 3:\n",
    "                label = ent[2]\n",
    "                if label not in added_labels:\n",
    "                    ner.add_label(label)\n",
    "                    added_labels.add(label)\n",
    "    \n",
    "    \n",
    "    pipe_exceptions = [\"ner\", \"tok2vec\"]\n",
    "    unaffected_pipes = [pipe for pipe in model.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "    with model.disable_pipes(*unaffected_pipes):\n",
    "        examples = []\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for idx, (text, annots) in enumerate(train_data):\n",
    "            try:\n",
    "                doc = model.make_doc(text)\n",
    "                \n",
    "                if len(doc) == 0:\n",
    "                    print(f\"Skipping example #{idx}: Empty document\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                invalid_spans = []\n",
    "                for start, end, label in annots.get(\"entities\", []):\n",
    "                    if start >= end or start < 0 or end > len(text):\n",
    "                        invalid_spans.append((start, end, label))\n",
    "                \n",
    "                if invalid_spans:\n",
    "                    print(f\"Skipping example #{idx}: Contains invalid spans {invalid_spans}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                example = Example.from_dict(doc, annots)\n",
    "                examples.append(example)\n",
    "                \n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping example #{idx}: {str(e)[:100]}...\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error in example #{idx}: {str(e)[:100]}...\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        if not examples:\n",
    "            raise ValueError(\"No valid training examples were created. Cannot train model.\")\n",
    "        \n",
    "        optimizer = model.begin_training()\n",
    "        for i in range(iterations):\n",
    "            random.shuffle(examples)\n",
    "            \n",
    "            batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
    "            \n",
    "            batch_losses = {}\n",
    "            for batch in batches:\n",
    "                try:\n",
    "                    model.update(\n",
    "                        batch,\n",
    "                        drop=0.5,\n",
    "                        sgd=optimizer,\n",
    "                        losses=batch_losses\n",
    "                    )\n",
    "                    for k, v in batch_losses.items():\n",
    "                        if k not in losses:\n",
    "                            losses[k] = [v]\n",
    "                        else:\n",
    "                            losses[k].append(v)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during batch update: {str(e)[:100]}...\")\n",
    "                    continue\n",
    "            \n",
    "            iter_loss = {}\n",
    "            for k, v in losses.items():\n",
    "                if v:\n",
    "                    iter_loss[k] = sum(v) / len(v)\n",
    "            \n",
    "            print(f\"Iteration {i+1}/{iterations}, Loss: {iter_loss}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data and calculate precision, recall, and F1 score.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained spaCy model\n",
    "        test_data: Test data in spaCy format\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for text, annotations in test_data:\n",
    "        true_entities = annotations.get(\"entities\", [])\n",
    "        true_entity_spans = set((start, end, label) for start, end, label in true_entities)\n",
    "        \n",
    "        doc = model(text)\n",
    "        pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        pred_entity_spans = set(pred_entities)\n",
    "        \n",
    "        true_positives += len(true_entity_spans.intersection(pred_entity_spans))\n",
    "        false_positives += len(pred_entity_spans - true_entity_spans)\n",
    "        false_negatives += len(true_entity_spans - pred_entity_spans)\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives\n",
    "    }\n",
    "\n",
    "def generate_output(model, texts, original_df=None):\n",
    "    \"\"\"\n",
    "    Generate entity predictions for a list of texts and create output dataframe.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained spaCy model\n",
    "        texts: List of text strings to analyze\n",
    "        original_df: Optional dataframe with original annotations for comparison\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with prediction results and optional comparison to ground truth\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        try:\n",
    "            doc = model(text)\n",
    "            \n",
    "            conditions = []\n",
    "            procedures = []\n",
    "            medications = []\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                normalized_text = ent.text.strip()\n",
    "                \n",
    "                if ent.label_ == \"Condition\" and normalized_text:\n",
    "                    conditions.append(normalized_text)\n",
    "                elif ent.label_ == \"Procedure\" and normalized_text:\n",
    "                    procedures.append(normalized_text)\n",
    "                elif ent.label_ == \"Medication\" and normalized_text:\n",
    "                    medications.append(normalized_text)\n",
    "            \n",
    "            conditions = list(dict.fromkeys(conditions))\n",
    "            procedures = list(dict.fromkeys(procedures))\n",
    "            medications = list(dict.fromkeys(medications))\n",
    "            \n",
    "            condition_str = ', '.join(conditions) if conditions else ''\n",
    "            procedure_str = ', '.join(procedures) if procedures else ''\n",
    "            medication_str = ', '.join(medications) if medications else ''\n",
    "            \n",
    "            result_row = {\n",
    "                'text': text,\n",
    "                'Condition': condition_str,\n",
    "                'Procedure': procedure_str,\n",
    "                'Medication': medication_str\n",
    "            }\n",
    "            \n",
    "            if original_df is not None and i < len(original_df):\n",
    "                result_row['original_Condition'] = ', '.join(original_df.iloc[i]['Condition']) if isinstance(original_df.iloc[i]['Condition'], list) else original_df.iloc[i]['Condition']\n",
    "                result_row['original_Procedure'] = ', '.join(original_df.iloc[i]['Procedure']) if isinstance(original_df.iloc[i]['Procedure'], list) else original_df.iloc[i]['Procedure']\n",
    "                result_row['original_Medication'] = ', '.join(original_df.iloc[i]['Medication']) if isinstance(original_df.iloc[i]['Medication'], list) else original_df.iloc[i]['Medication']\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text #{i}: {str(e)[:100]}...\")\n",
    "            result_row = {\n",
    "                'text': text,\n",
    "                'Condition': '',\n",
    "                'Procedure': '',\n",
    "                'Medication': ''\n",
    "            }\n",
    "            \n",
    "            if original_df is not None and i < len(original_df):\n",
    "                result_row['original_Condition'] = ', '.join(original_df.iloc[i]['Condition']) if isinstance(original_df.iloc[i]['Condition'], list) else original_df.iloc[i]['Condition']\n",
    "                result_row['original_Procedure'] = ', '.join(original_df.iloc[i]['Procedure']) if isinstance(original_df.iloc[i]['Procedure'], list) else original_df.iloc[i]['Procedure']\n",
    "                result_row['original_Medication'] = ', '.join(original_df.iloc[i]['Medication']) if isinstance(original_df.iloc[i]['Medication'], list) else original_df.iloc[i]['Medication']\n",
    "            \n",
    "            results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def evaluate_by_entity_type(model, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data and calculate metrics for each entity type.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained spaCy model\n",
    "        test_data: Test data in spaCy format\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics by entity type\n",
    "    \"\"\"\n",
    "    metrics_by_type = {\n",
    "        \"Condition\": {\"tp\": 0, \"fp\": 0, \"fn\": 0},\n",
    "        \"Procedure\": {\"tp\": 0, \"fp\": 0, \"fn\": 0},\n",
    "        \"Medication\": {\"tp\": 0, \"fp\": 0, \"fn\": 0}\n",
    "    }\n",
    "    \n",
    "    for text, annotations in test_data:\n",
    "        true_entities = annotations.get(\"entities\", [])\n",
    "        doc = model(text)\n",
    "        pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        true_by_type = {\n",
    "            \"Condition\": set(),\n",
    "            \"Procedure\": set(),\n",
    "            \"Medication\": set()\n",
    "        }\n",
    "        \n",
    "        for start, end, label in true_entities:\n",
    "            true_by_type[label].add((start, end))\n",
    "        \n",
    "        pred_by_type = {\n",
    "            \"Condition\": set(),\n",
    "            \"Procedure\": set(),\n",
    "            \"Medication\": set()\n",
    "        }\n",
    "        \n",
    "        for start, end, label in pred_entities:\n",
    "            if label in pred_by_type:\n",
    "                pred_by_type[label].add((start, end))\n",
    "        \n",
    "        for entity_type in [\"Condition\", \"Procedure\", \"Medication\"]:\n",
    "            true_spans = true_by_type[entity_type]\n",
    "            pred_spans = pred_by_type[entity_type]\n",
    "            \n",
    "            metrics_by_type[entity_type][\"tp\"] += len(true_spans.intersection(pred_spans))\n",
    "            metrics_by_type[entity_type][\"fp\"] += len(pred_spans - true_spans)\n",
    "            metrics_by_type[entity_type][\"fn\"] += len(true_spans - pred_spans)\n",
    "    \n",
    "    results = {}\n",
    "    for entity_type, counts in metrics_by_type.items():\n",
    "        tp = counts[\"tp\"]\n",
    "        fp = counts[\"fp\"]\n",
    "        fn = counts[\"fn\"]\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results[entity_type] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def spacy_strubell_model_main():\n",
    "    \"\"\"\n",
    "    Main function to train and evaluate the Strubell NER model for medical entities.\n",
    "    \n",
    "    Reads train/test data, trains model, evaluates performance, and generates output file.\n",
    "    \"\"\"\n",
    "    print(\"Starting Strubell model training..\")\n",
    "    X_train, y_train, X_test, y_test = read_train_test_split()\n",
    "    train_entities = []\n",
    "    for i in range(len(y_train)):\n",
    "        train_entities.append({\n",
    "            \"Condition\": y_train.iloc[i]['Condition'],\n",
    "            \"Procedure\": y_train.iloc[i]['Procedure'],\n",
    "            \"Medication\": y_train.iloc[i]['Medication']\n",
    "        })\n",
    "\n",
    "    test_entities = []\n",
    "    for i in range(len(y_test)):\n",
    "        test_entities.append({\n",
    "            \"Condition\": y_test.iloc[i]['Condition'],\n",
    "            \"Procedure\": y_test.iloc[i]['Procedure'],\n",
    "            \"Medication\": y_test.iloc[i]['Medication']\n",
    "        })\n",
    "\n",
    "    train_data = convert_to_spacy_format(X_train.tolist(), train_entities)\n",
    "    test_data = convert_to_spacy_format(X_test.tolist(), test_entities)\n",
    "\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe(\"ner\")\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    pipe_exceptions = [\"ner\", \"tok2vec\"]\n",
    "\n",
    "    trained_model = train_spacy_model(nlp, train_data, iterations=15)\n",
    "\n",
    "    overall_metrics = evaluate_model(trained_model, test_data)\n",
    "    entity_metrics = evaluate_by_entity_type(trained_model, test_data)\n",
    "\n",
    "    print(\"\\nOverall Model Performance:\")\n",
    "    print(f\"Precision: {overall_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {overall_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {overall_metrics['f1']:.4f}\")\n",
    "\n",
    "    print(\"\\nPerformance by Entity Type:\")\n",
    "    for entity_type, metrics in entity_metrics.items():\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "\n",
    "    output_df = generate_output(trained_model, X_test.tolist(), original_df=y_test)\n",
    "    output_df.to_csv(STRUBELL_MODEL_OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e92e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment to run the model\n",
    "# spacy_strubell_model_main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
