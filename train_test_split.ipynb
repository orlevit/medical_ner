{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.17.0\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3 (ipykernel)\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aaa5a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from config import DATA_FILE, TRAIN_TEST_OUTPUT_FILE\n",
    "from helper import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing special characters and excessive whitespace\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_stratified_split(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Creates a stratified split that ensures both common and rare entities \n",
    "    are distributed between training and test sets.\n",
    "    \n",
    "    The approach:\n",
    "    1. First include texts with rare entities in the training set\n",
    "    2. Create clusters based on text similarity for the remaining texts\n",
    "    3. Stratify the split based on these clusters\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    entity_counts = {}\n",
    "    for col in ['Condition', 'Procedure', 'Medication']:\n",
    "        for entities in df[col]:\n",
    "            for entity in entities:\n",
    "                if entity:\n",
    "                    entity_counts[entity] = entity_counts.get(entity, 0) + 1\n",
    "\n",
    "    rare_entities = {entity for entity, count in entity_counts.items() if count < 3}\n",
    "    print(f\"Number of rare entities (appearing less than 3 times): {len(rare_entities)}\")\n",
    "\n",
    "    contains_rare = []\n",
    "    for i, row in df.iterrows():\n",
    "        has_rare = False\n",
    "        for col in ['Condition', 'Procedure', 'Medication']:\n",
    "            if any(entity in rare_entities for entity in row[col] if entity):\n",
    "                has_rare = True\n",
    "                break\n",
    "        contains_rare.append(has_rare)\n",
    "    \n",
    "    df['contains_rare'] = contains_rare\n",
    "    rare_texts_df = df[df['contains_rare']].copy()\n",
    "    common_texts_df = df[~df['contains_rare']].copy()\n",
    "\n",
    "    print(f\"Texts with rare entities: {len(rare_texts_df)} ({len(rare_texts_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"Texts with common entities only: {len(common_texts_df)} ({len(common_texts_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    rare_train_df, rare_test_df = train_test_split(rare_texts_df, test_size=0.1, random_state=random_state)\n",
    "\n",
    "    if len(common_texts_df) > 0:\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        X_tfidf = vectorizer.fit_transform(common_texts_df['processed_text'])\n",
    "        n_clusters = min(max(5, len(common_texts_df) // 100), 20)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "        clusters = kmeans.fit_predict(X_tfidf)\n",
    "        common_texts_df['cluster'] = clusters\n",
    "        common_train_df, common_test_df = train_test_split(common_texts_df, test_size=test_size, stratify=common_texts_df['cluster'], random_state=random_state)\n",
    "    else:\n",
    "        common_train_df = pd.DataFrame(columns=df.columns)\n",
    "        common_test_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    train_df = pd.concat([rare_train_df, common_train_df])\n",
    "    test_df = pd.concat([rare_test_df, common_test_df])\n",
    "    \n",
    "    if 'contains_rare' in train_df.columns:\n",
    "        train_df = train_df.drop(columns=['contains_rare'])\n",
    "    if 'cluster' in train_df.columns:\n",
    "        train_df = train_df.drop(columns=['cluster'])\n",
    "    if 'contains_rare' in test_df.columns:\n",
    "        test_df = test_df.drop(columns=['contains_rare'])\n",
    "    if 'cluster' in test_df.columns:\n",
    "        test_df = test_df.drop(columns=['cluster'])\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def train_test_splitting():\n",
    "    if not os.path.exists(TRAIN_TEST_OUTPUT_FILE):\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "        df.fillna(value='', inplace=True)\n",
    "        \n",
    "        convert_string_to_list(df, 'Procedure')\n",
    "        convert_string_to_list(df, 'Condition')\n",
    "        convert_string_to_list(df, 'Medication')\n",
    "        \n",
    "        \n",
    "        df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "        \n",
    "        print(\"\\nAnalyzing entity cardinality:\")\n",
    "        all_conditions = [item for sublist in df['Condition'].tolist() for item in sublist if item]\n",
    "        all_procedures = [item for sublist in df['Procedure'].tolist() for item in sublist if item]\n",
    "        all_medications = [item for sublist in df['Medication'].tolist() for item in sublist if item]\n",
    "        \n",
    "        unique_conditions = set(all_conditions)\n",
    "        unique_procedures = set(all_procedures)\n",
    "        unique_medications = set(all_medications)\n",
    "        \n",
    "        print(f\"Data rows: {len(df)}\")\n",
    "        print(f\"Unique Conditions: {len(unique_conditions)}\")\n",
    "        print(f\"Unique Procedures: {len(unique_procedures)}\")\n",
    "        print(f\"Unique Medications: {len(unique_medications)}\")\n",
    "        \n",
    "        \n",
    "        train_df, test_df = create_stratified_split(df, test_size=0.2, random_state=42)\n",
    "            \n",
    "        X_train = train_df['processed_text']\n",
    "        y_train = train_df[['Condition', 'Procedure', 'Medication']]\n",
    "        X_test = test_df['processed_text']\n",
    "        y_test = test_df[['Condition', 'Procedure', 'Medication']]\n",
    "\n",
    "        data = {\n",
    "            \"X_train\": X_train.tolist(),\n",
    "            \"y_train\": y_train.to_dict(orient=\"records\"),\n",
    "            \"X_test\": X_test.tolist(),\n",
    "            \"y_test\": y_test.to_dict(orient=\"records\")\n",
    "        }\n",
    "        \n",
    "        with open(TRAIN_TEST_OUTPUT_FILE, 'w') as f:\n",
    "            json.dump(data, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
