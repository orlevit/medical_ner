{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98fdb82-fe03-425f-90c5-bf8c505a9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from helper import read_train_test_split, prepare_data_BIO\n",
    "from config import CRF_MODEL_OUTPUT_FILE\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def evaluate_crf_model(crf_model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a trained CRF model using test data and calculates performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        crf_model: Trained CRF model\n",
    "        X_test: Test features\n",
    "        y_test: Test labels in BIO format\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary containing precision, recall, and F1 scores for each entity type and overall\n",
    "        report: Classification report as string\n",
    "        y_pred: Predicted labels\n",
    "    \"\"\"\n",
    "    y_pred = crf_model.predict(X_test)\n",
    "    \n",
    "    y_true_flat = [label for sublist in y_test for label in sublist]\n",
    "    y_pred_flat = [label for sublist in y_pred for label in sublist]\n",
    "    \n",
    "    labels = set(y_true_flat) - {'O'}\n",
    "    sorted_labels = sorted(list(labels), key=lambda name: (name[1:], name[0]))\n",
    "    \n",
    "    report = flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=4)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    entity_types = set()\n",
    "    for label in sorted_labels:\n",
    "        entity_type = label[2:]\n",
    "        entity_types.add(entity_type)\n",
    "    \n",
    "    for entity_type in entity_types:\n",
    "        entity_labels = [label for label in sorted_labels if label.endswith(entity_type)]\n",
    "        \n",
    "        y_true_entity = ['1' if label.endswith(entity_type) else '0' for label in y_true_flat]\n",
    "        y_pred_entity = ['1' if label.endswith(entity_type) else '0' for label in y_pred_flat]\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_entity, y_pred_entity, average='binary', pos_label='1'\n",
    "        )\n",
    "        \n",
    "        results[entity_type] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true_flat, y_pred_flat, average='micro', labels=sorted_labels\n",
    "    )\n",
    "    \n",
    "    results['overall'] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    return results, report, y_pred\n",
    "\n",
    "def get_features_for_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Extracts features for all words in a sentence.\n",
    "    \n",
    "    Args:\n",
    "        sentence: List of spaCy tokens representing a sentence\n",
    "    \n",
    "    Returns:\n",
    "        List of feature dictionaries for each word in the sentence\n",
    "    \"\"\"\n",
    "    return [get_features_for_one_word(i, sentence) for i in range(len(sentence))]\n",
    "\n",
    "def get_features_for_one_word(cur_loc, sentence):\n",
    "    \"\"\"\n",
    "    Extracts features for a single word in a sentence.\n",
    "    \n",
    "    Features include word text, POS tag, dependency relation, head word,\n",
    "    suffix, capitalization, and contextual features from surrounding words.\n",
    "    \n",
    "    Args:\n",
    "        cur_loc: Index of the current word\n",
    "        sentence: List of spaCy tokens representing a sentence\n",
    "    \n",
    "    Returns:\n",
    "        List of features for the word\n",
    "    \"\"\"\n",
    "    end_loc = len(sentence) - 1\n",
    "    word = sentence[cur_loc]\n",
    "\n",
    "    word_text = word.text if hasattr(word, 'text') else word.orth_\n",
    "    word_pos = word.pos_ if hasattr(word, 'pos_') else 'NONE'\n",
    "    word_dep = word.dep_ if hasattr(word, 'dep_') else 'NONE'\n",
    "    \n",
    "    try:\n",
    "        head_text = word.head.text if hasattr(word.head, 'text') else word.head.orth_\n",
    "    except:\n",
    "        head_text = 'NONE'\n",
    "    \n",
    "    if len(word_text) >= 3:\n",
    "        last_three = word_text[-3:]\n",
    "    else:\n",
    "        last_three = word_text\n",
    "        \n",
    "    try:\n",
    "        starts_with_capital = word_text[0].isupper() if word_text else False\n",
    "    except:\n",
    "        starts_with_capital = False\n",
    "\n",
    "    features = [\n",
    "        f'word{0}.lower=' + word_text.lower(),\n",
    "        f'word{0}.postag=' + word_pos,\n",
    "        f'word{0}[-3:]=' + last_three,\n",
    "        f'word{0}.dep=' + word_dep,\n",
    "        f'word{0}.head=' + head_text,\n",
    "        f'word{0}.isupper={word_text.isupper()}',\n",
    "        f'word{0}.isdigit={word_text.isdigit()}',\n",
    "        f'word{0}.startsWithCapital={starts_with_capital}'\n",
    "    ]\n",
    "    \n",
    "    if cur_loc > 0:\n",
    "        prev_word = sentence[cur_loc - 1]\n",
    "        prev_word_text = prev_word.text if hasattr(prev_word, 'text') else prev_word.orth_\n",
    "        prev_word_pos = prev_word.pos_ if hasattr(prev_word, 'pos_') else 'NONE'\n",
    "        prev_word_dep = prev_word.dep_ if hasattr(prev_word, 'dep_') else 'NONE'\n",
    "        \n",
    "        try:\n",
    "            prev_head_text = prev_word.head.text if hasattr(prev_word.head, 'text') else prev_word.head.orth_\n",
    "        except:\n",
    "            prev_head_text = 'NONE'\n",
    "        \n",
    "        if len(prev_word_text) >= 3:\n",
    "            prev_last_three = prev_word_text[-3:]\n",
    "        else:\n",
    "            prev_last_three = prev_word_text\n",
    "            \n",
    "        try:\n",
    "            prev_starts_with_capital = prev_word_text[0].isupper() if prev_word_text else False\n",
    "        except:\n",
    "            prev_starts_with_capital = False\n",
    "        \n",
    "        features.extend([\n",
    "            f'word{-1}.lower=' + prev_word_text.lower(),\n",
    "            f'word{-1}.postag=' + prev_word_pos,\n",
    "            f'word{-1}[-3:]=' + prev_last_three,\n",
    "            f'word{-1}.dep=' + prev_word_dep,\n",
    "            f'word{-1}.head=' + prev_head_text,\n",
    "            f'word{-1}.isupper={prev_word_text.isupper()}',\n",
    "            f'word{-1}.isdigit={prev_word_text.isdigit()}',\n",
    "            f'word{-1}.startsWithCapital={prev_starts_with_capital}'\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BEG')\n",
    "\n",
    "    if cur_loc < end_loc:\n",
    "        next_word = sentence[cur_loc + 1]\n",
    "        next_word_text = next_word.text if hasattr(next_word, 'text') else next_word.orth_\n",
    "        next_word_pos = next_word.pos_ if hasattr(next_word, 'pos_') else 'NONE'\n",
    "        \n",
    "        if len(next_word_text) >= 3:\n",
    "            next_last_three = next_word_text[-3:]\n",
    "        else:\n",
    "            next_last_three = next_word_text\n",
    "            \n",
    "        try:\n",
    "            next_starts_with_capital = next_word_text[0].isupper() if next_word_text else False\n",
    "        except:\n",
    "            next_starts_with_capital = False\n",
    "        \n",
    "        features.extend([\n",
    "            f'word{1}.lower=' + next_word_text.lower(),\n",
    "            f'word{1}.postag=' + next_word_pos,\n",
    "            f'word{1}[-3:]=' + next_last_three,\n",
    "            f'word{1}.isdigit={next_word_text.isdigit()}',\n",
    "            f'word{1}.startsWithCapital={next_starts_with_capital}'\n",
    "        ])\n",
    "    \n",
    "    if cur_loc == end_loc:\n",
    "        features.append('END')\n",
    "\n",
    "    return features\n",
    "\n",
    "def train_crf_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Trains a Conditional Random Field (CRF) model for named entity recognition.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels in BIO format\n",
    "    \n",
    "    Returns:\n",
    "        Trained CRF model\n",
    "    \"\"\"\n",
    "    crf = CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    \n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    return crf\n",
    "\n",
    "def convert_predictions_to_entities(texts, y_pred):\n",
    "    \"\"\"\n",
    "    Converts BIO tag predictions into entity dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of original text strings\n",
    "        y_pred: Predicted BIO tags from the CRF model\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with extracted entities grouped by entity type\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        doc = nlp(text)\n",
    "        labels = y_pred[i]\n",
    "        \n",
    "        entity_dict = {\n",
    "            'Condition': [],\n",
    "            'Procedure': [],\n",
    "            'Medication': []\n",
    "        }\n",
    "        \n",
    "        current_entity = None\n",
    "        current_type = None\n",
    "        current_start = None\n",
    "        \n",
    "        for j, (token, label) in enumerate(zip(doc, labels)):\n",
    "            if label.startswith('B-'):\n",
    "                if current_entity is not None:\n",
    "                    entity_dict[current_type].append(current_entity)\n",
    "                \n",
    "                current_type = label[2:]\n",
    "                current_entity = token.text\n",
    "                current_start = j\n",
    "            \n",
    "            elif label.startswith('I-'):\n",
    "                if current_entity is not None and label[2:] == current_type:\n",
    "                    current_entity += ' ' + token.text\n",
    "                else:\n",
    "                    current_type = label[2:]\n",
    "                    current_entity = token.text\n",
    "                    current_start = j\n",
    "            \n",
    "            elif label == 'O' and current_entity is not None:\n",
    "                entity_dict[current_type].append(current_entity)\n",
    "                current_entity = None\n",
    "                current_type = None\n",
    "                current_start = None\n",
    "        \n",
    "        if current_entity is not None:\n",
    "            entity_dict[current_type].append(current_entity)\n",
    "        \n",
    "        result.append(entity_dict)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def generate_output_adjusted(texts, predicted_entity_dicts, original_df=None):\n",
    "    \"\"\"\n",
    "    Generates a DataFrame with predicted entities and optionally original annotations.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of original text strings\n",
    "        predicted_entity_dicts: List of dictionaries with predicted entities\n",
    "        original_df: Optional DataFrame containing original entity annotations\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns for text, predicted entities, and original entities if provided\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        try:\n",
    "            condition_str = ', '.join(predicted_entity_dicts[i]['Condition']) if predicted_entity_dicts[i]['Condition'] else ''\n",
    "            procedure_str = ', '.join(predicted_entity_dicts[i]['Procedure']) if predicted_entity_dicts[i]['Procedure'] else ''\n",
    "            medication_str = ', '.join(predicted_entity_dicts[i]['Medication']) if predicted_entity_dicts[i]['Medication'] else ''\n",
    "            \n",
    "            result_row = {\n",
    "                'text': text,\n",
    "                'Condition': condition_str,\n",
    "                'Procedure': procedure_str,\n",
    "                'Medication': medication_str\n",
    "            }\n",
    "            \n",
    "            if original_df is not None and i < len(original_df):\n",
    "                result_row['original_Condition'] = ', '.join(original_df.iloc[i]['Condition']) if isinstance(original_df.iloc[i]['Condition'], list) else original_df.iloc[i]['Condition']\n",
    "                result_row['original_Procedure'] = ', '.join(original_df.iloc[i]['Procedure']) if isinstance(original_df.iloc[i]['Procedure'], list) else original_df.iloc[i]['Procedure']\n",
    "                result_row['original_Medication'] = ', '.join(original_df.iloc[i]['Medication']) if isinstance(original_df.iloc[i]['Medication'], list) else original_df.iloc[i]['Medication']\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text #{i}: {str(e)[:100]}...\")\n",
    "            result_row = {\n",
    "                'text': text,\n",
    "                'Condition': '',\n",
    "                'Procedure': '',\n",
    "                'Medication': ''\n",
    "            }\n",
    "            \n",
    "            if original_df is not None and i < len(original_df):\n",
    "                result_row['original_Condition'] = ', '.join(original_df.iloc[i]['Condition']) if isinstance(original_df.iloc[i]['Condition'], list) else original_df.iloc[i]['Condition']\n",
    "                result_row['original_Procedure'] = ', '.join(original_df.iloc[i]['Procedure']) if isinstance(original_df.iloc[i]['Procedure'], list) else original_df.iloc[i]['Procedure']\n",
    "                result_row['original_Medication'] = ', '.join(original_df.iloc[i]['Medication']) if isinstance(original_df.iloc[i]['Medication'], list) else original_df.iloc[i]['Medication']\n",
    "            \n",
    "            results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def run_crf_pipeline_with_original_columns(X_train, y_train_entity_dicts, X_test, y_test_entity_dicts, y_test_df=None):\n",
    "    \"\"\"\n",
    "    Runs the complete CRF pipeline: training, evaluation, and prediction.\n",
    "    \n",
    "    Args:\n",
    "        X_train: List of training texts\n",
    "        y_train_entity_dicts: Training entity annotations\n",
    "        X_test: List of test texts\n",
    "        y_test_entity_dicts: Test entity annotations\n",
    "        y_test_df: Optional DataFrame with original test annotations\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing the model, evaluation metrics, predictions, and output DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Preparing training data for CRF...\")\n",
    "    X_train_features, y_train_labels = prepare_data_BIO(X_train, y_train_entity_dicts, get_features_for_sentence)\n",
    "    \n",
    "    print(\"Training CRF model...\")\n",
    "    crf_model = train_crf_model(X_train_features, y_train_labels)\n",
    "    \n",
    "    print(\"Preparing test data for CRF...\")\n",
    "    X_test_features, y_test_labels = prepare_data_BIO(X_test, y_test_entity_dicts, get_features_for_sentence)\n",
    "    \n",
    "    print(\"Evaluating CRF model...\")\n",
    "    eval_results, report, y_pred = evaluate_crf_model(crf_model, X_test_features, y_test_labels)\n",
    "    \n",
    "    print(\"Converting predictions to entity format...\")\n",
    "    pred_entity_dicts = convert_predictions_to_entities(X_test, y_pred)\n",
    "    \n",
    "    print(\"Generating output dataframe with original columns...\")\n",
    "    output_df = generate_output_adjusted(X_test, pred_entity_dicts, original_df=y_test_df)\n",
    "    \n",
    "    return {\n",
    "        'model': crf_model,\n",
    "        'evaluation': eval_results,\n",
    "        'report': report,\n",
    "        'predictions': pred_entity_dicts,\n",
    "        'output_df': output_df\n",
    "    }\n",
    "\n",
    "def crf_main():\n",
    "    \"\"\"\n",
    "    Main function to run the CRF model pipeline.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads train and test data\n",
    "    2. Prepares entity dictionaries from the data\n",
    "    3. Runs the CRF pipeline\n",
    "    4. Saves the output to a CSV file\n",
    "    5. Prints detailed performance metrics\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test = read_train_test_split()\n",
    "    train_entities = []\n",
    "    for i in range(len(y_train)):\n",
    "        train_entities.append({\n",
    "            \"Condition\": y_train.iloc[i]['Condition'],\n",
    "            \"Procedure\": y_train.iloc[i]['Procedure'],\n",
    "            \"Medication\": y_train.iloc[i]['Medication']\n",
    "        })\n",
    "\n",
    "    test_entities = []\n",
    "    for i in range(len(y_test)):\n",
    "        test_entities.append({\n",
    "            \"Condition\": y_test.iloc[i]['Condition'],\n",
    "            \"Procedure\": y_test.iloc[i]['Procedure'],\n",
    "            \"Medication\": y_test.iloc[i]['Medication']\n",
    "        })\n",
    "\n",
    "    test_df = pd.DataFrame({\n",
    "        'Condition': y_test['Condition'],\n",
    "        'Procedure': y_test['Procedure'],\n",
    "        'Medication': y_test['Medication']\n",
    "    })\n",
    "\n",
    "    results = run_crf_pipeline_with_original_columns(\n",
    "        X_train.tolist(), \n",
    "        train_entities, \n",
    "        X_test.tolist(), \n",
    "        test_entities,\n",
    "        y_test_df=test_df\n",
    "    )\n",
    "\n",
    "    results['output_df'].to_csv(CRF_MODEL_OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(results['report'])\n",
    "\n",
    "    for entity_type in ['Condition', 'Procedure', 'Medication']:\n",
    "        if entity_type in results['evaluation']:\n",
    "            print(f\"\\n{entity_type}:\")\n",
    "            print(f\"F1 Score: {results['evaluation'][entity_type]['f1']:.4f}\")\n",
    "            print(f\"Precision: {results['evaluation'][entity_type]['precision']:.4f}\")\n",
    "            print(f\"Recall: {results['evaluation'][entity_type]['recall']:.4f}\")\n",
    "\n",
    "    print(\"\\nCRF Model Performance:\")\n",
    "    print(f\"Overall F1 Score: {results['evaluation']['overall']['f1']:.4f}\")\n",
    "    print(f\"Overall Precision: {results['evaluation']['overall']['precision']:.4f}\")\n",
    "    print(f\"Overall Recall: {results['evaluation']['overall']['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899d927-d8de-43e1-b460-b84557a3bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment to run the model\n",
    "# crf_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f66cf-72e2-4f52-97b9-d944c3d01a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
