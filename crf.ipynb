{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d98fdb82-fe03-425f-90c5-bf8c505a9d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data for CRF...\n",
      "Training CRF model...\n",
      "Preparing test data for CRF...\n",
      "Evaluating CRF model...\n",
      "Converting predictions to entity format...\n",
      "Generating output dataframe...\n",
      "\n",
      "CRF Model Performance:\n",
      "Overall F1 Score: 0.9504\n",
      "Overall Precision: 0.9561\n",
      "Overall Recall: 0.9449\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " B-Condition     0.9701    0.9619    0.9660     16817\n",
      " I-Condition     0.8920    0.8737    0.8828      8157\n",
      "B-Medication     1.0000    0.9754    0.9876       244\n",
      "I-Medication     1.0000    0.9630    0.9811        54\n",
      " B-Procedure     0.9947    0.9879    0.9913      4562\n",
      " I-Procedure     0.9914    0.9792    0.9853      2594\n",
      "\n",
      "   micro avg     0.9561    0.9449    0.9504     32428\n",
      "   macro avg     0.9747    0.9569    0.9657     32428\n",
      "weighted avg     0.9559    0.9449    0.9503     32428\n",
      "\n",
      "\n",
      "Condition:\n",
      "F1 Score: 0.9454\n",
      "Precision: 0.9513\n",
      "Recall: 0.9395\n",
      "\n",
      "Procedure:\n",
      "F1 Score: 0.9901\n",
      "Precision: 0.9945\n",
      "Recall: 0.9857\n",
      "\n",
      "Medication:\n",
      "F1 Score: 0.9864\n",
      "Precision: 1.0000\n",
      "Recall: 0.9732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Assuming X_train, y_train, X_test, y_test are already defined\\n# Convert y_train and y_test to the expected format (list of dicts with entity types as keys)\\ntrain_entities = []\\nfor i in range(len(y_train)):\\n    train_entities.append({\\n        \"Condition\": y_train.iloc[i][\\'Condition\\'],\\n        \"Procedure\": y_train.iloc[i][\\'Procedure\\'],\\n        \"Medication\": y_train.iloc[i][\\'Medication\\']\\n    })\\n\\ntest_entities = []\\nfor i in range(len(y_test)):\\n    test_entities.append({\\n        \"Condition\": y_test.iloc[i][\\'Condition\\'],\\n        \"Procedure\": y_test.iloc[i][\\'Procedure\\'],\\n        \"Medication\": y_test.iloc[i][\\'Medication\\']\\n    })\\n\\n# Run the CRF pipeline\\nresults = run_crf_pipeline(X_train.tolist(), train_entities, X_test.tolist(), test_entities)\\n\\n# Print evaluation results\\nprint(\"\\nCRF Model Performance:\")\\nprint(f\"Overall F1 Score: {results[\\'evaluation\\'][\\'overall\\'][\\'f1\\']:.4f}\")\\nprint(f\"Overall Precision: {results[\\'evaluation\\'][\\'overall\\'][\\'precision\\']:.4f}\")\\nprint(f\"Overall Recall: {results[\\'evaluation\\'][\\'overall\\'][\\'recall\\']:.4f}\")\\n\\nprint(\"\\nDetailed Classification Report:\")\\nprint(results[\\'report\\'])\\n\\nfor entity_type in [\\'Condition\\', \\'Procedure\\', \\'Medication\\']:\\n    if entity_type in results[\\'evaluation\\']:\\n        print(f\"\\n{entity_type}:\")\\n        print(f\"F1 Score: {results[\\'evaluation\\'][entity_type][\\'f1\\']:.4f}\")\\n        print(f\"Precision: {results[\\'evaluation\\'][entity_type][\\'precision\\']:.4f}\")\\n        print(f\"Recall: {results[\\'evaluation\\'][entity_type][\\'recall\\']:.4f}\")\\n\\n# Save output to CSV\\nresults[\\'output_df\\'].to_csv(\\'medical_ner_crf_results.csv\\', index=False)\\nprint(\"\\nOutput saved to \\'medical_ner_crf_results.csv\\'\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b899d927-d8de-43e1-b460-b84557a3bd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data for CRF...\n",
      "Training CRF model...\n",
      "Preparing test data for CRF...\n",
      "Evaluating CRF model...\n",
      "Converting predictions to entity format...\n",
      "Generating output dataframe with original columns...\n",
      "\n",
      "CRF Model Performance:\n",
      "Overall F1 Score: 0.9504\n",
      "Overall Precision: 0.9561\n",
      "Overall Recall: 0.9449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Load your data\\nX_train, y_train, X_test, y_test = read_train_test_split()\\n\\n# Prepare entity lists for training\\ntrain_entities = []\\nfor i in range(len(y_train)):\\n    train_entities.append({\\n        \"Condition\": y_train.iloc[i][\\'Condition\\'],\\n        \"Procedure\": y_train.iloc[i][\\'Procedure\\'],\\n        \"Medication\": y_train.iloc[i][\\'Medication\\']\\n    })\\n\\n# Prepare entity lists for testing\\ntest_entities = []\\nfor i in range(len(y_test)):\\n    test_entities.append({\\n        \"Condition\": y_test.iloc[i][\\'Condition\\'],\\n        \"Procedure\": y_test.iloc[i][\\'Procedure\\'],\\n        \"Medication\": y_test.iloc[i][\\'Medication\\']\\n    })\\n\\n# Create test DataFrame for original columns\\ntest_df = pd.DataFrame({\\n    \\'Condition\\': y_test[\\'Condition\\'],\\n    \\'Procedure\\': y_test[\\'Procedure\\'],\\n    \\'Medication\\': y_test[\\'Medication\\']\\n})\\n\\n# Run the pipeline with original columns\\nresults = run_crf_pipeline_with_original_columns(\\n    X_train.tolist(), \\n    train_entities, \\n    X_test.tolist(), \\n    test_entities,\\n    y_test_df=test_df\\n)\\n\\n# Print evaluation results\\nprint(\"\\nCRF Model Performance:\")\\nprint(f\"Overall F1 Score: {results[\\'evaluation\\'][\\'overall\\'][\\'f1\\']:.4f}\")\\nprint(f\"Overall Precision: {results[\\'evaluation\\'][\\'overall\\'][\\'precision\\']:.4f}\")\\nprint(f\"Overall Recall: {results[\\'evaluation\\'][\\'overall\\'][\\'recall\\']:.4f}\")\\n\\n# Display a few rows to compare predicted vs original entities\\nprint(\"\\nSample of output with predicted and original entities:\")\\nprint(results[\\'output_df\\'][[\\'text\\', \\'Condition\\', \\'original_Condition\\']].head(3))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_output_adjusted(texts, predicted_entity_dicts, original_df=None):\n",
    "    \"\"\"\n",
    "    Generate output dataframe in the required format with both predicted and original entity columns\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts\n",
    "        predicted_entity_dicts: List of dictionaries with predicted entity types as keys and lists of entities as values\n",
    "        original_df: Original DataFrame with entity columns to consolidate\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: text, predicted entities, and original entities\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        try:\n",
    "            # Convert predicted entity lists to comma-separated strings\n",
    "            condition_str = ', '.join(predicted_entity_dicts[i]['Condition']) if predicted_entity_dicts[i]['Condition'] else ''\n",
    "            procedure_str = ', '.join(predicted_entity_dicts[i]['Procedure']) if predicted_entity_dicts[i]['Procedure'] else ''\n",
    "            medication_str = ', '.join(predicted_entity_dicts[i]['Medication']) if predicted_entity_dicts[i]['Medication'] else ''\n",
    "            \n",
    "            # Create result row with predicted entities\n",
    "            result_row = {\n",
    "                'text': text,\n",
    "                'Condition': condition_str,\n",
    "                'Procedure': procedure_str,\n",
    "                'Medication': medication_str\n",
    "            }\n",
    "            \n",
    "            # Add original entity columns if original_df is provided\n",
    "            if original_df is not None and i < len(original_df):\n",
    "                # Get original values\n",
    "                result_row['original_Condition'] = ', '.join(original_df.iloc[i]['Condition']) if isinstance(original_df.iloc[i]['Condition'], list) else original_df.iloc[i]['Condition']\n",
    "                result_row['original_Procedure'] = ', '.join(original_df.iloc[i]['Procedure']) if isinstance(original_df.iloc[i]['Procedure'], list) else original_df.iloc[i]['Procedure']\n",
    "                result_row['original_Medication'] = ', '.join(original_df.iloc[i]['Medication']) if isinstance(original_df.iloc[i]['Medication'], list) else original_df.iloc[i]['Medication']\n",
    "            \n",
    "            results.append(result_row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text #{i}: {str(e)[:100]}...\")\n",
    "            # Add an empty row with just the text to maintain alignment\n",
    "            result_row = {\n",
    "                'text': text,\n",
    "                'Condition': '',\n",
    "                'Procedure': '',\n",
    "                'Medication': ''\n",
    "            }\n",
    "            \n",
    "            # Add original entity columns if original_df is provided\n",
    "            if original_df is not None and i < len(original_df):\n",
    "                result_row['original_Condition'] = ', '.join(original_df.iloc[i]['Condition']) if isinstance(original_df.iloc[i]['Condition'], list) else original_df.iloc[i]['Condition']\n",
    "                result_row['original_Procedure'] = ', '.join(original_df.iloc[i]['Procedure']) if isinstance(original_df.iloc[i]['Procedure'], list) else original_df.iloc[i]['Procedure']\n",
    "                result_row['original_Medication'] = ', '.join(original_df.iloc[i]['Medication']) if isinstance(original_df.iloc[i]['Medication'], list) else original_df.iloc[i]['Medication']\n",
    "            \n",
    "            results.append(result_row)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Update the run_crf_pipeline function to use the adjusted output function\n",
    "def run_crf_pipeline_with_original_columns(X_train, y_train_entity_dicts, X_test, y_test_entity_dicts, y_test_df=None):\n",
    "    \"\"\"\n",
    "    Run the complete CRF pipeline with consolidated output\n",
    "    \n",
    "    Args:\n",
    "        X_train: List of training texts\n",
    "        y_train_entity_dicts: List of dictionaries with training entity annotations\n",
    "        X_test: List of test texts\n",
    "        y_test_entity_dicts: List of dictionaries with test entity annotations\n",
    "        y_test_df: Original test DataFrame with entity columns\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results and output dataframe\n",
    "    \"\"\"\n",
    "    print(\"Preparing training data for CRF...\")\n",
    "    X_train_features, y_train_labels = prepare_data_for_crf(X_train, y_train_entity_dicts)\n",
    "    \n",
    "    print(\"Training CRF model...\")\n",
    "    crf_model = train_crf_model(X_train_features, y_train_labels)\n",
    "    \n",
    "    print(\"Preparing test data for CRF...\")\n",
    "    X_test_features, y_test_labels = prepare_data_for_crf(X_test, y_test_entity_dicts)\n",
    "    \n",
    "    print(\"Evaluating CRF model...\")\n",
    "    eval_results, report, y_pred = evaluate_crf_model(crf_model, X_test_features, y_test_labels)\n",
    "    \n",
    "    print(\"Converting predictions to entity format...\")\n",
    "    pred_entity_dicts = convert_predictions_to_entities(X_test, y_pred)\n",
    "    \n",
    "    print(\"Generating output dataframe with original columns...\")\n",
    "    output_df = generate_output_adjusted(X_test, pred_entity_dicts, original_df=y_test_df)\n",
    "    \n",
    "    return {\n",
    "        'model': crf_model,\n",
    "        'evaluation': eval_results,\n",
    "        'report': report,\n",
    "        'predictions': pred_entity_dicts,\n",
    "        'output_df': output_df\n",
    "    }\n",
    "\n",
    "X_train, y_train, X_test, y_test = read_train_test_split()\n",
    "# Prepare entity lists for training\n",
    "train_entities = []\n",
    "for i in range(len(y_train)):\n",
    "    train_entities.append({\n",
    "        \"Condition\": y_train.iloc[i]['Condition'],\n",
    "        \"Procedure\": y_train.iloc[i]['Procedure'],\n",
    "        \"Medication\": y_train.iloc[i]['Medication']\n",
    "    })\n",
    "\n",
    "# Prepare entity lists for testing\n",
    "test_entities = []\n",
    "for i in range(len(y_test)):\n",
    "    test_entities.append({\n",
    "        \"Condition\": y_test.iloc[i]['Condition'],\n",
    "        \"Procedure\": y_test.iloc[i]['Procedure'],\n",
    "        \"Medication\": y_test.iloc[i]['Medication']\n",
    "    })\n",
    "\n",
    "# Create test DataFrame for original columns\n",
    "test_df = pd.DataFrame({\n",
    "    'Condition': y_test['Condition'],\n",
    "    'Procedure': y_test['Procedure'],\n",
    "    'Medication': y_test['Medication']\n",
    "})\n",
    "\n",
    "# Run the pipeline with original columns\n",
    "results = run_crf_pipeline_with_original_columns(\n",
    "    X_train.tolist(), \n",
    "    train_entities, \n",
    "    X_test.tolist(), \n",
    "    test_entities,\n",
    "    y_test_df=test_df\n",
    ")\n",
    "\n",
    " \n",
    "# Save the output directly\n",
    "results['output_df'].to_csv('medical_ner_crf_results_with_original.csv', index=False)\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(results['report'])\n",
    "\n",
    "for entity_type in ['Condition', 'Procedure', 'Medication']:\n",
    "    if entity_type in results['evaluation']:\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        print(f\"F1 Score: {results['evaluation'][entity_type]['f1']:.4f}\")\n",
    "        print(f\"Precision: {results['evaluation'][entity_type]['precision']:.4f}\")\n",
    "        print(f\"Recall: {results['evaluation'][entity_type]['recall']:.4f}\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nCRF Model Performance:\")\n",
    "print(f\"Overall F1 Score: {results['evaluation']['overall']['f1']:.4f}\")\n",
    "print(f\"Overall Precision: {results['evaluation']['overall']['precision']:.4f}\")\n",
    "print(f\"Overall Recall: {results['evaluation']['overall']['recall']:.4f}\")\n",
    "\n",
    "\n",
    "# Example usage of the updated function\n",
    "\"\"\"\n",
    "# Load your data\n",
    "X_train, y_train, X_test, y_test = read_train_test_split()\n",
    "\n",
    "# Prepare entity lists for training\n",
    "train_entities = []\n",
    "for i in range(len(y_train)):\n",
    "    train_entities.append({\n",
    "        \"Condition\": y_train.iloc[i]['Condition'],\n",
    "        \"Procedure\": y_train.iloc[i]['Procedure'],\n",
    "        \"Medication\": y_train.iloc[i]['Medication']\n",
    "    })\n",
    "\n",
    "# Prepare entity lists for testing\n",
    "test_entities = []\n",
    "for i in range(len(y_test)):\n",
    "    test_entities.append({\n",
    "        \"Condition\": y_test.iloc[i]['Condition'],\n",
    "        \"Procedure\": y_test.iloc[i]['Procedure'],\n",
    "        \"Medication\": y_test.iloc[i]['Medication']\n",
    "    })\n",
    "\n",
    "# Create test DataFrame for original columns\n",
    "test_df = pd.DataFrame({\n",
    "    'Condition': y_test['Condition'],\n",
    "    'Procedure': y_test['Procedure'],\n",
    "    'Medication': y_test['Medication']\n",
    "})\n",
    "\n",
    "# Run the pipeline with original columns\n",
    "results = run_crf_pipeline_with_original_columns(\n",
    "    X_train.tolist(), \n",
    "    train_entities, \n",
    "    X_test.tolist(), \n",
    "    test_entities,\n",
    "    y_test_df=test_df\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nCRF Model Performance:\")\n",
    "print(f\"Overall F1 Score: {results['evaluation']['overall']['f1']:.4f}\")\n",
    "print(f\"Overall Precision: {results['evaluation']['overall']['precision']:.4f}\")\n",
    "print(f\"Overall Recall: {results['evaluation']['overall']['recall']:.4f}\")\n",
    "\n",
    "# Display a few rows to compare predicted vs original entities\n",
    "print(\"\\nSample of output with predicted and original entities:\")\n",
    "print(results['output_df'][['text', 'Condition', 'original_Condition']].head(3))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c102402-edf6-47d8-bd21-298306e56c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " B-Condition     0.9701    0.9619    0.9660     16817\n",
      " I-Condition     0.8920    0.8737    0.8828      8157\n",
      "B-Medication     1.0000    0.9754    0.9876       244\n",
      "I-Medication     1.0000    0.9630    0.9811        54\n",
      " B-Procedure     0.9947    0.9879    0.9913      4562\n",
      " I-Procedure     0.9914    0.9792    0.9853      2594\n",
      "\n",
      "   micro avg     0.9561    0.9449    0.9504     32428\n",
      "   macro avg     0.9747    0.9569    0.9657     32428\n",
      "weighted avg     0.9559    0.9449    0.9503     32428\n",
      "\n",
      "\n",
      "Condition:\n",
      "F1 Score: 0.9454\n",
      "Precision: 0.9513\n",
      "Recall: 0.9395\n",
      "\n",
      "Procedure:\n",
      "F1 Score: 0.9901\n",
      "Precision: 0.9945\n",
      "Recall: 0.9857\n",
      "\n",
      "Medication:\n",
      "F1 Score: 0.9864\n",
      "Precision: 1.0000\n",
      "Recall: 0.9732\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(results['report'])\n",
    "\n",
    "for entity_type in ['Condition', 'Procedure', 'Medication']:\n",
    "    if entity_type in results['evaluation']:\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        print(f\"F1 Score: {results['evaluation'][entity_type]['f1']:.4f}\")\n",
    "        print(f\"Precision: {results['evaluation'][entity_type]['precision']:.4f}\")\n",
    "        print(f\"Recall: {results['evaluation'][entity_type]['recall']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f056d635-33db-4b05-8d0f-8eabdf131c14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
